---
title: "ETC3250/5250 Assignment 2"
author: "Michael Wang"
date: "2024-04-12"
quarto-required: ">=1.3.0"
format:
    html:
        output-file: assign02-submission.html
        css: "assignment.css"
        embed-resources: true
---

<!-- Guide to using quarto at https://quarto.org/docs/get-started/hello/rstudio.html -->

```{r, include=FALSE}
library(MASS)
library(readr)
library(tidyverse)
library(mulgar)
library(ggplot2)
library(plotly)
library(gridExtra)
library(knitr)
library(kableExtra)
library(GGally)
library(vip)
library(tidymodels)
library(patchwork)
library(boot)
library(flextable)
library(yardstick)
library(discrim)
library(nullabor)
library(rpart.plot)
```

## Exercises

#### 1. Bootstrapping and permuting your way to provide evidence (12pts)

**Part a**

```{r, include=FALSE}
# define functions
set.seed(1000)
engwt20 <- read_csv("engwt20.csv")
engwt20_filtered <- engwt20 %>% 
  select(-Player, -Country, -Start, -End, -HighScoreNotOut, -Matches, 
         -InningsBowled, -Overs, -InningsBatted, -FiveWickets, -Hundreds)
pc <- prcomp(engwt20_filtered, center=TRUE, scale=TRUE)

# compute pc1, flipping the 10th variable for largest separation
compute_PC1 <- function(data, index, col) {
  pc <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,col]
  # Coordinate signs
  if (sign(pc[10]) < 0) 
    pc <- -pc 
  return(pc)
}

# compute pc2, flipping the first variable for largest separation
compute_PC2 <- function(data, index, col) {
  pc <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,col]
  # Coordinate signs
  if (sign(pc[1]) < 0) 
    pc <- -pc 
  return(pc)
}
```

1. PC1

- In the plot, the red line indicates positive average contribution and the blue line indicates negative average contribution.

- We can see that not outs, runs scored, high score, batting average, fifties and ducks have their confidence intervals including and above the red line, indicating that they primarily contribute to PC1. 

- The confidence interval of other variables include 0, meaning they are indistinguishable from 0 and are not primary contributors of PC1. 

- Among the non-primary contributors, maidens, runs conceeded, wickets and four wickets have more significant contributions than bowling average, economy and bowling strike rate as their confidence interval includes the average contribution line. 

- This interpretation makes sense as we expect PC1 to explain all the player performance statistics, which includes both batting and bowling statistics. 

*Numerical Analysis*

```{r, echo=FALSE}
pc1 <- pc$rotation[,1]
pc1 %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Visual Analysis*

```{r, echo=FALSE}
# analyze PC1
PC1_boot <- boot(engwt20_filtered, compute_PC1, R=1000, col=1)
colnames(PC1_boot$t) <- colnames(engwt20_filtered[,])

PC1_boot_ci <- as_tibble(PC1_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("Maidens", "RunsConceeded", "Wickets", "BowlingAverage", "Economy", "BowlingStrikeRate", "FourWickets",
                                    "NotOuts", "RunsScored", "HighScore", "BattingAverage", "Fifties", "Ducks"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC1_boot$t0) 

# used to determine which variable to flip
PC1_points <- as_tibble(PC1_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("Maidens", "RunsConceeded", "Wickets", "BowlingAverage", "Economy", "BowlingStrikeRate", "FourWickets",
                                    "NotOuts", "RunsScored", "HighScore", "BattingAverage", "Fifties", "Ducks"))) 
ggplot(PC1_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=1/sqrt(13), linetype=2, colour="red") +
  geom_hline(yintercept=-1/sqrt(13), linetype=2, colour="blue") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  # geom_jitter(data= PC1_points, aes(y = coef), alpha = 0.1)
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  labs(x = "variables", y = "coefficient") + 
  ggtitle("Bootstrapped PCA Plot of PC1")
```

2. PC2

- In the plot, the red line indicates positive average contribution and the blue line indicates negative average contribution.

- Maidens, runs conceeded, wickets, bowling average, economy and bowling strike rate lie above the positive average variance contribution line, which indicates that they primarily contribute to PC2. 

- The 95% confidence interval of not outs, runs scored, high score, batting average, fifties and ducks contains 0, meaning they are indistinguishable from 0 and are not primary contributors of PC2.

- Among the non-primary contributors, runs scored, high score, batting average and fifties have more significant contributions than bowling average, economy and bowling strike rate as their confidence interval includes the negative average contribution line. 

- This interpretation makes sense as we expect PC2 to explain the difference between batting experts and bowling experts.

*Numerical Analysis*

```{r, echo=FALSE}
pc2 <- pc$rotation[,2]
pc2 %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Visual Analysis*

```{r, echo=FALSE}
# analyse PC2
PC2_boot <- boot(engwt20_filtered, compute_PC2, R=1000, col=2)
colnames(PC2_boot$t) <- colnames(engwt20_filtered[,])

PC2_boot_ci <- as_tibble(PC2_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("Maidens", "RunsConceeded", "Wickets", "BowlingAverage", "Economy", "BowlingStrikeRate", "FourWickets",
                                    "NotOuts", "RunsScored", "HighScore", "BattingAverage", "Fifties", "Ducks"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC2_boot$t0) 

# used to determine which variable to flip
PC2_points <- as_tibble(PC2_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("Maidens", "RunsConceeded", "Wickets", "BowlingAverage", "Economy", "BowlingStrikeRate", "FourWickets",
                                    "NotOuts", "RunsScored", "HighScore", "BattingAverage", "Fifties", "Ducks")))
         
ggplot(PC2_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=1/sqrt(13), linetype=2, colour="red") +
  geom_hline(yintercept=-1/sqrt(13), linetype=2, colour="blue") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  # geom_jitter(data= PC2_points, aes(y = coef), alpha = 0.1)
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  labs(x = "variables", y = "coefficient") + 
  ggtitle("Bootstrapped PCA Plot of PC2")
```

**Part b**

1. Runs Scored & High Score

- The data and the permuted data for runs scored and high score are very different. The permutation breaks any relationship between the two variables, so we know that there is no relationship in any of the permuted data examples. This says that the relationship seen in the data is statistically significant.

- This can be verified by the correlation of the original data (0.85), which is very close to 1. The plot of the original data clearly shows some positive relationship, which is broken in the permutations.

```{r, include=FALSE}
set.seed(1000)
df <- engwt20 %>% 
  select(RunsScored, HighScore)
df_l <- lineup(null_permute('RunsScored'), df)
```

*Original Data*

```{r, echo=FALSE}
ggplot(df, aes(x=RunsScored, y=HighScore)) +
  geom_point() + 
  ggtitle("Scatter Plot of Original Data")

paste("Correlation of the original data is:", cor(df$RunsScored, df$HighScore))
```

*Permuted Data*

```{r, echo=FALSE}
# test relationship between runs scored and high score, graph look similar so they are statistically insignificant 
ggplot(df_l, aes(x=RunsScored, y=HighScore)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  facet_wrap(~.sample) + 
  ggtitle("Scatter Plot of Permuted Samples")

df_l %>% 
  group_by(.sample) %>% 
  summarise(correlation = cor(RunsScored, HighScore)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

2. Runs Scored & Runs Conceeded

- The data and the permuted data for runs scored and runs conceeded are very similar. The permutation breaks any relationship between the two variables, so we know that there is no relationship in any of the permuted data examples. This says that the relationship seen in the data is statistically insignificant.

- This can be verified by the correlation of each sample shown in the table for each permuted sample, and the correlation of the original data, which have values very close to 0, indicate little to no correlation. 

```{r, include=FALSE}
set.seed(1000)
df <- engwt20 %>% 
  select(RunsScored, RunsConceeded)
df_l <- lineup(null_permute('RunsScored'), df)
```

*Original Data*

```{r, echo=FALSE}
ggplot(df, aes(x=RunsScored, y=RunsConceeded)) +
  geom_point() + 
  ggtitle("Scatter Plot of Original Data")

paste("Correlation of the original data is:", cor(df$RunsScored, df$RunsConceeded))
```

*Permuted Data*

```{r, echo=FALSE}
# test relationship between runs scored and runs conceeded, graph look similar so they are statistically insignificant 
ggplot(df_l, aes(x=RunsScored, y=RunsConceeded)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  facet_wrap(~.sample)

df_l %>% 
  group_by(.sample) %>% 
  summarise(correlation = cor(RunsScored, RunsConceeded)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

#### 2. How well can you build a simple classifier? (16pts)

```{r, include=FALSE}
finance_birds <- read_csv("./finance_and_birds.csv")
finance_birds <- finance_birds %>%
  select(type, linearity, entropy, x_acf1, covariate1, covariate2)
finance_birds$type <- as.factor(finance_birds$type)
non_standard <- finance_birds
# standardize data
finance_birds <- finance_birds %>%
  mutate(across(where(is.numeric), ~as.numeric(scale(.))))
```

**Part a**

1. Data Summary/Distinguish

- From the summary statistics of each class, we can see that there is a notable difference between linearity, entropy and covariate2 between the two types of time series. For instance, finance shows much higher linearity (mean of 15.38) compared to birdsongs (mean of -0.04). Finance time series also has a higher autocorrelation (x_acf1) and a lower entropy, indicating that finance data might follow more predictable and linear patterns, while birdsongs have more complex and less predictable behaviors.

- Covariate1 shows similar values between birdsongs and finance, suggesting it may not be a strong differentiator between the two types of time series. Covariate2 has a higher mean for finance (2.49) compared to birdsongs (1.02), suggesting that covariate2 might be useful in distinguishing between these two types of time series. It might represent some external factor influencing the finance data differently than the birdsong data.

2. LDA Assumptions

- One of the key assumptions for LDA is that each feature must follow a normal distribution when viewed individually for each class. Looking at the density plot for each feature distinguished by class, we see that apart from covariate1 and covariate2, the other features do not seem to follow a normal distribution. This is validated by the output of the Shapiro-Wilk test for normality for each of the predictors in each class, which shows that the null hypothesis (feature follows a normal distribution) is rejected for all but covariate1, covariate2 and birdsongs entropy. Hence, this assumption is violated. 

- The second assumption is that each feature has the same variance-covariance matrix for all classes. Visually, this means ellipses for each class should have similar shapes/spread in each pairwise scatter plot. We can see that this assumption is also violated as the ellipses in each plot don't have similar shapes/spread and the covariance for some pairs are very different for the two classes. For example, the covariance of entropy and linearity is 0.055 for birdsongs and -0.539 for finance. 

*Data Summary by Class*

```{r, echo=FALSE}
non_standard %>%
  group_by(type) %>%
  summarise(across(c(linearity, entropy, x_acf1, covariate1, covariate2), 
                   list(mean = mean, sd = sd))) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Density Plot + Pairwise Scatter Plot*

```{r, echo=FALSE}
# predictors have the same variance-covariance matrix (similar shape/spread of ellipse)
ggpairs(finance_birds[,-1], 
        aes(color = finance_birds$type, alpha = 0.5)) +
  theme_minimal()
```

*Perform Normality Test for each feature*

```{r, echo=FALSE}
finance_birds %>%
  group_by(type) %>%
  summarise(
    linearity_shapiro = shapiro.test(linearity)$p.value,
    entropy_shapiro = shapiro.test(entropy)$p.value,
    x_acf1_shapiro = shapiro.test(x_acf1)$p.value,
    covariate1_shapiro = shapiro.test(covariate1)$p.value,
    covariate2_shapiro = shapiro.test(covariate2)$p.value
  ) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

**Part b**

```{r}
set.seed(1000)
splits <- initial_split(finance_birds, prop = 0.7, strata = type)

# training 
fb_tr <- training(splits)

# testing
fb_ts <- testing(splits)
```

**Part c**

1. LDA

```{r, include=FALSE}
# LDA
lda_type <- discrim_linear() |>
  set_mode("classification") |>
  set_engine("MASS", prior = c(1/2, 1/2))

# fit model using training set
lda_fit <- lda_type |> 
  fit(type ~ ., data = fb_tr)
```

*Variable Importance*

```{r, echo=FALSE}
lda_fit$fit$scaling %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Training Set Evaluation*

```{r, echo=FALSE}
# try model on training set
lda_tr_pred <- fb_tr |>
  mutate(ptype = predict(lda_fit$fit, fb_tr)$class) 

# evaluate testing set
lda_tr_pred |> count(type, ptype) |>
  group_by(type) |>
  mutate(cl_acc = n[ptype == type]/sum(n)) |>
  pivot_wider(names_from = ptype, 
              values_from = n, values_fill=0) |>
  select(type, finance, birdsongs, cl_acc) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(lda_tr_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

# visualize training set
lda_tr_pred_prob <- lda_fit |> 
  augment(new_data = fb_tr, type.predict="prob") |> 
  mutate(.pred_correct = as.numeric(type == "birdsongs")*.pred_birdsongs + as.numeric(type == "finance")*.pred_finance) |>
  mutate(.pred_predicted = as.numeric(.pred_class == "birdsongs")*.pred_birdsongs + as.numeric(.pred_class == "finance")*.pred_finance)

ggplot(lda_tr_pred_prob, aes(x = type, y = .pred_correct)) +
  geom_boxplot() + 
  ggtitle("Training Set Prediction Probabilities with LDA Model")
```

*Testing Set Evaluation*

```{r, echo=FALSE}
# try model on testing set 
lda_ts_pred <- fb_ts |>
  mutate(ptype = predict(lda_fit$fit, fb_ts)$class)

# evaluate testing set
lda_ts_pred |> count(type, ptype) |>
  group_by(type) |>
  mutate(cl_acc = n[ptype == type]/sum(n)) |>
  pivot_wider(names_from = ptype, 
              values_from = n, values_fill=0) |>
  select(type, finance, birdsongs, cl_acc) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(lda_ts_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

# visualize testing set 
lda_ts_pred_prob <- lda_fit |> 
  augment(new_data = fb_ts, type.predict="prob") |> 
  mutate(.pred_correct = as.numeric(type == "birdsongs")*.pred_birdsongs + as.numeric(type == "finance")*.pred_finance) |>
  mutate(.pred_predicted = as.numeric(.pred_class == "birdsongs")*.pred_birdsongs + as.numeric(.pred_class == "finance")*.pred_finance)

ggplot(lda_ts_pred_prob, aes(x = type, y = .pred_correct)) +
  geom_boxplot() + 
  ggtitle("Testing Set Prediction Probabilities with LDA Model")
```

2. Logistic Regression

*Model Summary*

```{r, echo=FALSE}
# Logistic Regression
log_fit <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification") |> 
  translate()

# fit model on training set
log_tr_pred <- log_fit %>% 
  fit(type~., data = fb_tr)

# examine model fit
glance(log_tr_pred) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Variable Importance*

```{r, echo=FALSE}
log_tr_pred$fit
```

*Training Set Evaluation*

```{r, echo=FALSE}
# evaluate training set
log_tr_fit <- augment(log_tr_pred, fb_tr)

log_tr_fit |> count(type, .pred_class) |>
  group_by(type) |>
  mutate(cl_acc = n[.pred_class == type]/sum(n)) |>
  pivot_wider(names_from = .pred_class, 
              values_from = n, values_fill=0) |>
  select(type, finance, birdsongs, cl_acc) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(log_tr_fit, type, .pred_class) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

# visualize training set
log_tr_pred_prob <- log_tr_fit |> 
  mutate(.pred_correct = as.numeric(type == "birdsongs")*.pred_birdsongs + as.numeric(type == "finance")*.pred_finance) |> 
  mutate(.pred_predicted = as.numeric(.pred_class == "birdsongs")*.pred_birdsongs + as.numeric(.pred_class == "finance")*.pred_finance) 

ggplot(log_tr_pred_prob, aes(x = type, y = .pred_correct)) +
  geom_boxplot() + 
  ggtitle("Training Set Prediction Probabilities with Logistic Regression Model")
```

*Testing Set Evaluation*

```{r, echo=FALSE}
# evaluate testing set 
log_ts_fit <- augment(log_tr_pred, fb_ts)

log_ts_fit |> count(type, .pred_class) |>
  group_by(type) |>
  mutate(cl_acc = n[.pred_class == type]/sum(n)) |>
  pivot_wider(names_from = .pred_class, 
              values_from = n, values_fill=0) |>
  select(type, finance, birdsongs, cl_acc) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(log_ts_fit, type, .pred_class) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

# visualize testing set 
log_ts_pred_prob <- log_ts_fit |> 
  mutate(.pred_correct = as.numeric(type == "birdsongs")*.pred_birdsongs + as.numeric(type == "finance")*.pred_finance) |> 
  mutate(.pred_predicted = as.numeric(.pred_class == "birdsongs")*.pred_birdsongs + as.numeric(.pred_class == "finance")*.pred_finance) 

ggplot(log_ts_pred_prob, aes(x = type, y = .pred_correct)) +
  geom_boxplot() + 
  ggtitle("Testing Set Prediction Probabilities with Logistic Regression Model")
```

**Part d**

1. Confusion Matrices

- The LDA model has a balanced accuracy of 0.817 for both the training and testing set.  

- The logistic regression model has a balanced accuracy of 0.816 for the training set and 0.823 for the testing set. 

- Both models show relatively similar confusion matrices. In both the training and testing set, LDA appears to have a higher accuracy when predicting birdsongs (0.85/0.88 for LDA compared to 0.83/0.86 for logistic regression), and logistic regression model appears to have a higher accuracy when predicting finance (0.80/0.79 for logistic regression compared to 0.78/0.75 for LDA).

- The logistic regression model has a higher balanced accuracy in the testing set (0.823 vs 0.817) but lower balanced accuracy in the training set (0.816 vs 0.817).

- Looking at the model evaluation for logistic regression, the model deviance decreased by 400 compared to the null, which meant that the predictors explained the outcome better than the null model. 

2. Mistakes & Box Plot 

- For both the LDA and logistic regression model, the box for birdsongs is shorter than finance, which indicates lower variability and more consistent predictions.

- The median probability for birdsongs and finance are similar for the LDA model, while the logistic regression model has a higher median for finance than birdsongs. This means the logistic regression model assigns higher probability to finance when it gets it right. 

- Both models struggle more with finance predictions as it has a lower prediction accuracy and more variability than birdsongs. 

3. Variable Contribution 

- For both the LDA and logistic regression model, covariate2 (1.03/1.81), entropy (-0.41/-0.75) and linearity (0.37/0.86) have the most influential coefficients, meaning they are primary contributors when trying to separate the two time series.

**Part e**

-> Assuming birdsongs is the positive class:

- The ROC curves of both models (logistic regression and LDA) show very similar AUC values on both the training and testing sets, indicating comparable performance in terms of overall discrimination ability.

- However, the logistic regression model achieves a higher balanced accuracy on the testing set, and unlike LDA, it doesn't assume normality or equal class covariances, making it more robust in general.

- Therefore, based on both the ROC AUC and balanced accuracy, logistic regression is likely the better model overall.

*LDA Model*

```{r, echo=FALSE}
lda_roc_tr <- roc_curve(lda_tr_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - LDA (Training Set)")

lda_roc_ts <- roc_curve(lda_ts_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - LDA (Testing Set)")

tr_auc <- lda_tr_pred_prob %>% roc_auc(type, .pred_birdsongs)
ts_auc <- lda_ts_pred_prob %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

lda_roc_tr + lda_roc_ts
```

*Logistic Regression Model*

```{r, echo=FALSE}
log_roc_tr <- roc_curve(log_tr_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Logistic (Training Set)")

log_roc_ts <- roc_curve(log_ts_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Logistic (Testing Set)")

tr_auc <- log_tr_pred_prob %>% roc_auc(type, .pred_birdsongs)
ts_auc <- log_ts_pred_prob %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

log_roc_tr + log_roc_ts
```

**Part f**

- When analyzing time series data for financial data and birdsongs, we see distinct differences in their characteristics. 

- Financial time series tend to exhibit more linear behavior with higher autocorrelation and lower entropy, suggesting that financial data may follow more predictable patterns over time. This is reinforced by the coefficients of variables in the logistic regression model. We see that entropy (measure of unpredictability) and linearity (how well model can be modelled using linear relationship) have very high influence on the model, indicating that they are primary contributing factors when trying to distinguish between the two time series. Similarly, the LDA model also have linearity and entropy to be important contributors out of all variables in separating the time series.

- covariate2 also appears to be a primary contributing factor with the second highest coefficient in both models, meaning it is also an important variable in separating the two time series. This may represent a time series feature derived from the structure or dynamics of time series data that help to distinguish different time series.

- The high linearity and autocorrelation values of finance indicate a smoother, more structured progression in the data, which is characteristic of financial markets influenced by economic trends, news, and market sentiment. In contrast, birdsong time series show much lower linearity and higher entropy, pointing to more complex and less predictable behavior. Birdsong data often follows cyclic or periodic patterns, influenced by biological and environmental factors, leading to more irregular and less structured time series. 

- Thus, while finance data is more stable and predictable, birdsong data shows more variability and complexity, influenced by natural rhythms and environmental factors.

#### 3. Tuning a non-linear classifier (12pts)

**Part a**

*Naive Decision Tree*

```{r, echo=FALSE, warning=FALSE}
tree_spec <- decision_tree(
  cost_complexity = 0,
  min_n = 1
) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

fb_fit_tree <- tree_spec %>% 
  fit(type~., data=fb_tr)

fb_fit_tree |>
  extract_fit_engine() |>
  rpart.plot(type=3, extra=1)
```

*Training Set Evaluation*

```{r, echo=FALSE}
# evaluate trainign set
fb_tr_pred <- fb_tr |>
  mutate(ptype = predict(fb_fit_tree$fit, 
                            fb_tr, 
                            type="class"))

fb_tr_pred |>
  count(type, ptype) |>
  group_by(type) |>
  mutate(Accuracy = n[type==ptype]/sum(n)) |>
  pivot_wider(names_from = "ptype", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_tr_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Testing Set Evaluation*

```{r, echo=FALSE}
# evaluate testing set
fb_ts_pred <- fb_ts |>
  mutate(ptype = predict(fb_fit_tree$fit, 
                            fb_ts, 
                            type="class"))

fb_ts_pred |>
  count(type, ptype) |>
  group_by(type) |>
  mutate(Accuracy = n[type==ptype]/sum(n)) |>
  pivot_wider(names_from = "ptype", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_ts_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

**Part b**

Optimal Hyperparamters:

```{r, echo=FALSE}
set.seed(1000)

tune_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

# create all combinations of hyperparameters, 5 values for each
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = 5)

tree_wf <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_formula(type~.)

# create 5 folds, each fold with one fold removed
fb_fold <- vfold_cv(fb_tr, v = 5, strata = type)

tree_res <- tree_wf %>% 
  tune_grid(
    resamples = fb_fold,
    grid = tree_grid,
    metrics = NULL
  )

tree_res %>% 
  select_best(metric = "roc_auc") %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

**Part c**

- Looking at the balanced accuracy for testing sets, the optimized decision tree model (0.88) has the higher accuracy than both the LDA model (0.817) and the logistic regression model (0.823). The naive decision tree also has higher balanced accuracy (0.86) than both LDA and the logistic regression model. 

- Looking at the ROC-AUC, the optimized decision tree model again has higher AUC (0.95, shown in question 4c) than both the LDA model (0.890) and the logistic regression model (0.889).

- In practice, it is important to weigh the trade offs in terms of performance and complexity. While decision trees are flexible and can model complex relationships, offering high accuracy, it is prone to over-fitting if hyper parameters are not tuned with cross validation. This is evident in the drop in accuracy from training to testing set (1 to 0.86) in the naive decision tree model. Moreover, the process of tuning can be time consuming or require considerable computational resources. 

- In contrast, LDA is simpler and works well when data meets the assumptions, but may underperform when the assumptions are violated. Logistic regression doesn't have any assumptions and performs well for linearly separable data. However, both models may be less accurate than decision trees when the relationship in the data are more complex or non-linear.

- In conclusion, logistic regression and LDA (if assumptions are satisfied) can offer a good balance between performance and simplicity if computational resource and time are limited, or when data has a linear relationship; while decision trees can provide higher accuracy with complex data at the cost of interpretability and computational demands. 

*Optimized Decision Tree*

```{r, echo=FALSE, warning=FALSE}
tree_spec <- decision_tree(
  cost_complexity = 1e-10,
  min_n = 21,
  tree_depth = 8
) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

fb_fit_tree <- tree_spec %>% 
  fit(type~., data=fb_tr)

fb_fit_tree |>
  extract_fit_engine() |>
  rpart.plot(type=3, extra=1)
```

*Training Set Evaluation*

```{r, echo=FALSE}
# evaluate trainign set
fb_tr_pred <- fb_tr |>
  mutate(ptype = predict(fb_fit_tree$fit, 
                            fb_tr, 
                            type="class"))

fb_tr_pred |>
  count(type, ptype) |>
  group_by(type) |>
  mutate(Accuracy = n[type==ptype]/sum(n)) |>
  pivot_wider(names_from = "ptype", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_tr_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

*Testing Set Evaluation*

```{r, echo=FALSE}
# evaluate testing set
fb_ts_pred <- fb_ts |>
  mutate(ptype = predict(fb_fit_tree$fit, 
                            fb_ts, 
                            type="class"))

fb_ts_pred |>
  count(type, ptype) |>
  group_by(type) |>
  mutate(Accuracy = n[type==ptype]/sum(n)) |>
  pivot_wider(names_from = "ptype", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_ts_pred, type, ptype) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

#### 4. Which is the better classifier? (15pts)

**Part a**

*Random Forest*

```{r, echo=FALSE}
set.seed(1000)

rf_spec <- rand_forest(mtry=2, trees=1000) |>
  set_mode("classification") |>
  set_engine("randomForest")

fb_fit_rf <- rf_spec |> 
  fit(type ~ ., data = fb_tr)

fb_preds <- predict(fb_fit_rf, fb_ts) |> 
  bind_cols(fb_ts) 

fb_fit_rf$fit

fb_preds |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[type==.pred_class]/sum(n)) |>
  pivot_wider(names_from = ".pred_class", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_preds, type, .pred_class) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

**Part b**

*Boosted Tree*

```{r, echo=FALSE}
set.seed(1000)

bt_spec <- boost_tree() |>
  set_mode("classification") |>
  set_engine("xgboost")

fb_fit_bt <- bt_spec |> 
  fit(type ~ ., data = fb_tr)

fb_preds <- predict(fb_fit_bt, fb_ts) |> 
  bind_cols(fb_ts)

fb_fit_bt$fit

fb_preds |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[type==.pred_class]/sum(n)) |>
  pivot_wider(names_from = ".pred_class", 
              values_from = n) |>
  select(type, birdsongs, finance, Accuracy) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")

bal_accuracy(fb_preds, type, .pred_class) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

**Part c**

Based on the AUC for testing set predictions, we can reach the conclusion of Boosted Tree > Random Forest > Tree > LDA > Logistic Regression.

*LDA Model*

```{r, echo=FALSE}
lda_roc_tr <- roc_curve(lda_tr_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - LDA (Training)")

lda_roc_ts <- roc_curve(lda_ts_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - LDA (Testing)")

tr_auc <- lda_tr_pred_prob %>% roc_auc(type, .pred_birdsongs)
ts_auc <- lda_ts_pred_prob %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

lda_roc_tr + lda_roc_ts
```

*Logistic Regression Model*

```{r, echo=FALSE}
log_roc_tr <- roc_curve(log_tr_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Logistic (Training)")

log_roc_ts <- roc_curve(log_ts_pred_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Logistic (Testing)")

tr_auc <- log_tr_pred_prob %>% roc_auc(type, .pred_birdsongs)
ts_auc <- log_ts_pred_prob %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

log_roc_tr + log_roc_ts
```

*Decision Tree with Optimal Hyperparameters*

```{r, echo=FALSE}
# training
fb_dt_tr_prob <- predict(fb_fit_tree, fb_tr, type="prob") %>% 
  bind_cols(fb_tr)

dt_roc_tr <- roc_curve(fb_dt_tr_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Tree (Training)")

# testing
fb_dt_ts_prob <- predict(fb_fit_tree, fb_ts, type="prob") %>% 
  bind_cols(fb_ts)

dt_roc_ts <- roc_curve(fb_dt_ts_prob, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Tree (Testing)")

tr_auc <- fb_dt_tr_prob %>% roc_auc(type, .pred_birdsongs)
ts_auc <- fb_dt_ts_prob %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

dt_roc_tr + dt_roc_ts
```

*Random Forest*

```{r, echo=FALSE}
# training 
fb_rf_tr <- predict(fb_fit_rf, fb_tr, type = 'prob') |> 
  bind_cols(fb_tr)

rf_roc_tr <- roc_curve(fb_rf_tr, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Random Forest (Training)")

# testing
fb_rf_ts <- predict(fb_fit_rf, fb_ts, type = 'prob') |> 
  bind_cols(fb_ts)

rf_roc_ts <- roc_curve(fb_rf_ts, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Random Forest (Testing)")

tr_auc <- fb_rf_tr %>% roc_auc(type, .pred_birdsongs)
ts_auc <- fb_rf_ts %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

rf_roc_tr + rf_roc_ts
```

*Boosted Tree*

```{r, echo=FALSE}
# training 
fb_bt_tr <- predict(fb_fit_bt, fb_tr, type = 'prob') |> 
  bind_cols(fb_tr)

bt_roc_tr <- roc_curve(fb_bt_tr, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Boosted Tree (Training)")

# testing 
fb_bt_ts <- predict(fb_fit_bt, fb_ts, type = 'prob') |> 
  bind_cols(fb_ts)

bt_roc_ts <- roc_curve(fb_bt_ts, type, .pred_birdsongs) %>% 
  autoplot() +
  ggtitle("ROC Curve - Boosted Tree (Testing)")

tr_auc <- fb_bt_tr %>% roc_auc(type, .pred_birdsongs)
ts_auc <- fb_bt_ts %>% roc_auc(type, .pred_birdsongs)

paste("The AUC for training set is", tr_auc['.estimate'])
paste("The AUC for testing set is", ts_auc['.estimate'])

bt_roc_tr + bt_roc_ts
```

**Part d**

Best Model Selection:

- Based on the balanced accuracy and roc-auc of the testing set, the best model is random forest with 0.881 (highest) balanced accuracy and 0.954 (second highest) ROC-AUC. 

- Balanced accuracy/ROC-AUC (testing set) of each model: 

```{r, echo=FALSE}
model_summary <- tibble(
  model = c("LDA", "Logistic Regression", "Optimized Decision Tree", "Random Forest", "Boosted Tree"),
  bal_acc = c(0.817, 0.823, 0.878, 0.881, 0.870),
  roc_auc = c(0.890, 0.889, 0.953, 0.954, 0.957)
)

model_summary %>%
  arrange(desc(bal_acc), desc(roc_auc)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
  
```

Distinguish finance vs birdsongs:

- Important variables for each model:
  
  -> LDA: covariate2 (1.027), entropy (-0.413), linearity (0.365)
  
  -> Logistic Regression: covariate2 (1.807), linearity (0.855), entropy (-0.749)
  
  -> Optimized Decision Tree: covariate2, linearity
  
  -> Random Forest: covariate2 (108.02), linearity (106.78), entropy (55.03)
  
  -> Boosted Tree: linearity, covariate2
  
- Across all models, covariate2 and linearity consistently emerged as the most influential features in distinguishing financial time series from birdsongs. In both LDA and logistic regression, covariate2 had the highest coefficients, indicating strong separation power. Random forest and decision tree models also identified covariate2 and linearity as the most important variables, with boosted trees highlighting linearity as the top contributor.

- These patterns suggest that financial time series tend to exhibit different structural characteristics compared to birdsongs. For instance, financial data may demonstrate more predictable or constrained patterns (higher linearity), while birdsongs may have more irregular, complex, or entropic behavior. The strong influence of entropy in some models (especially LDA and logistic regression) also supports this idea, indicating that birdsongs may contain more randomness or variability than financial series.

*Examine Variable Importance for All Models*

1. Logistic Regression Model

```{r, echo=FALSE}
log_tr_pred$fit
```

2. LDA Model

```{r, echo=FALSE}
lda_fit$fit$scaling %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

3. Optimized Decision Tree

Split by covariate2 -> linearity -> x_acf1 -> entropy.

4. Random Forest

```{r, echo=FALSE}
fb_fit_rf$fit$importance %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = TRUE, 
                position = "center")
```

5. Boosted Tree

```{r, echo=FALSE}
vip(fb_fit_bt) 
```

## References

Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer.

Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.

Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., ... & Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686.

Xie, Y. (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC.

Milborrow, S. (2023). rpart.plot: Plot 'rpart' Models. R package version 4.2.1.

Greenwell, B. M., & Boehmke, B. C. (2020). Variable Importance Plots—An Introduction to the vip Package. The R Journal, 12(1), 343–366.

Kuhn, M., & Vaughan, D. (2023). yardstick: Tidy Characterizations of Model Performance. R package.

Sievert, C. (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC.
  
OpenAI (2023). ChatGPT (version 3.5) [Large language model]. https://chat.openai.com/chat, full script of conversation [here]( https://chat.openai.com/share/746c4f77-9b7b-4c3b-867a-826f639b91aa)